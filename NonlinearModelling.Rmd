---
title: "Nonlinear Modelling"
output: html_notebook
---

```{r}
library(nlme)
library(ggplot2)
set.seed(10)
prostate = get(load("prostate2.Rdata"))
prostate$svi = as.factor(prostate$svi)
attach(prostate)
options(digits=3)
```

# Exploring the association between Cscore and lpsa

```{r}
defaultfit = lm(Cscore ~ lpsa, data = prostate)
plot(Cscore ~ lpsa)
abline(defaultfit)
```

```{r}
summary(defaultfit)
```

It can be seen that the trend between Cscore and lpsa does not appear perfectly linear, just by visual inspection. Some summary statistics confirm that a simple linear model only explains about half of the variance in this association.

Having confirmed our suspicion that there may be some non-linearity involved, we can turn to more powerful diagnostics to determine more conclusively what is happening.

```{r}
plot(defaultfit)
```

The plot of residuals against fitted values clearly resembles a parabola more than a straight line expected of a dataset with good linearity. Thus we strongly suspect that there is a large deviation from our assumption of linearity.

The Q-Q plot appears much less concerning, but there is still a notable parabolic relationship. The tails of the plot are clearly edging upwards, especially in the top half, where many data points of concern are located. Data point number 96 is particularly problematic. The Q-Q plot thus clearly indicates a small departure from the assumption of normality.

The scale-location plot again demonstrates a strong parabolic pattern, although there is a good spread of points - but again the problematic data points at the higher end are highlighted. Thus we strongly suspect that the residuals are not spread equally along the predictor ranges, and thus the assumption of homoscedasticity is violated as well.

The last diagnostic plot, residuals against leverage, shows that data point 96 is indeed a very influential case, and thus we would probably alter our model significantly if we exluded it from analysis. Data points 97 and 1 are also relatively influential cases.

Thus we can conclude rather confidently that most of the assumptions underlying linearity do not hold up very well for this association.


# Nonlinear modelling

First we randomise the rows and split the data set into test and validation subsets.

```{r}
sample = sample.int(n = nrow(prostate), size = floor(.5*nrow(prostate)), replace = F)
train = prostate[sample,]
test = prostate[-sample,]
```


## Polynomial regression

We can attempt to construct a range of different degree polynomial regression models. We will do this up to the 9th degree polynomial.

```{r}
poly_list = list()
MSE_list = list()
for (i in 1:9){
  model = lm(Cscore~poly(lpsa,i), data=prostate, subset=sample)
  poly_list = c(poly_list, list(model))
  mean = mean((Cscore-predict(model,prostate))[-sample]^2)
  MSE_list = c(MSE_list, list(mean))
}
best_poly_model = poly_list[[which.min(MSE_list)]]
```

As can be seen, the model that has the best test MSE is a third degree polynomial model. We can further confirm this using an ANOVA method for picking the best model.

```{r}
eval(parse(text=paste("anova(",paste("poly_list[[",1:length(poly_list),"]]",sep="",collapse=","),")")))
```

Model 3 is a significant improvement over Model 2, whereas Model 4 gives no significant improvement under a 0.05 significant threshold. Thus a second or third degree polynomial model provides a good fit to the data, and using our previous test MSE result we can pick the third degree polynomial as the best model with some confidence.

```{r}
fit = lm(Cscore~poly(lpsa, 3), data = prostate)

lpsalims = range(lpsa)
lpsa.grid = seq(lpsalims[1], lpsalims[2], length.out = 50)
prediction = predict(fit, newdata = list(lpsa = lpsa.grid), se = T)
SE.bands = cbind(prediction$fit+2*prediction$se.fit, prediction$fit-2*prediction$se.fit)

plot(lpsa, Cscore, xlim=lpsalims)
title("Third degree polynomial")
lines(lpsa.grid, prediction$fit)
matlines(lpsa.grid, SE.bands, lwd=1, col="red", lty=2)
```

## Cubic splines

A cubic spline will always have at least four degrees of freedom, since we have the three variables and at least one knot. We let the knots be put at uniform quantiles.

```{r}
library(splines)

cubic_list = c()
MSE_list = c()
for (i in 4:10){
  model = lm(Cscore~bs(lpsa, df = i),data=prostate, subset=sample)
  cubic_list = c(cubic_list, model)
  mean = mean((Cscore - predict(model, prostate))[-sample]^2)
  MSE_list = c(MSE_list, list(mean))
}
best_cubic_model = cubic_list[[which.min(MSE_list)]]
best_cubic_model
```

```{r}
fit = lm(Cscore~bs(lpsa, df = 4),data=prostate, subset=sample)

lpsalims = range(lpsa)
lpsa.grid = seq(lpsalims[1], lpsalims[2], length.out = 50)
prediction = predict(fit, newdata = list(lpsa = lpsa.grid), se = T)
SE.bands = cbind(prediction$fit+2*prediction$se.fit, prediction$fit-2*prediction$se.fit)

plot(lpsa, Cscore, xlim=lpsalims)
title("Single knot cubic spline")
lines(lpsa.grid, prediction$fit)
matlines(lpsa.grid, SE.bands, lwd=1, col="red", lty=2)
```


## Smoothing Splines

Since we have dulicated values present in lpsa, we can use generalised cross validation to arrive at the best possible value of the tuning parameter, with best defined as the lowest cross-validated RSS.

```{r}
fit = smooth.spline(lpsa, Cscore, cv = F)
fit$df
```

Unfortunately the best possible degrees of freedom value here is quite large, and we would like to constrain ourselves to a maximum of 10 degrees of freedom.

```{r}
lpsalims = range(lpsa)
lpsa.grid = seq(lpsalims[1], lpsalims[2], length.out = 50)
prediction = predict(fit, newdata = list(lpsa = lpsa.grid))

smooth_list = c()
MSE_list = c()
for (i in 2:10){
  model = smooth.spline(train$lpsa, train$Cscore,  df= i)
  smooth_list = c(smooth_list, list(model))
  lpsalims = range(train$lpsa)
  lpsa.grid = seq(lpsalims[1], lpsalims[2], length.out = length(Cscore))
  prediction = predict(model, newdata = list(lpsa = lpsa.grid))
  mean = mean((Cscore - prediction$y)[-sample]^2)
  MSE_list = c(MSE_list, mean)
}
best_smooth_model = smooth_list[[which.min(MSE_list)]]
##smoothspline_CV = smooth.spline( Cscore[sample],lpsa[sample], cv = TRUE)
##mean((Cscore-predict(smoothspline_CV, prostate))[-sample]^2)
```

