---
title: "Nonlinear Modelling"
output: html_notebook
---

```{r}
library(nlme)
library(ggplot2)
set.seed(10)
prostate = get(load("prostate2.Rdata"))
prostate$svi = as.factor(prostate$svi)
attach(prostate)
options(digits=3)
```

# Exploring the association between Cscore and lpsa

First we randomise the rows and split the data set into a training and validation subset.
```{r}
sample = sample.int(n = nrow(prostate), size = floor(.5*nrow(prostate)), replace = F)
train = prostate[sample,]
test = prostate[-sample,]
```

```{r}
defaultfit = lm(Cscore ~ lpsa, data = prostate, subset = sample)
plot(Cscore ~ lpsa)
abline(defaultfit)
```

```{r}
summary(defaultfit)
```

It can be seen that the trend between Cscore and lpsa does not appear perfectly linear, just by visual inspection. Some summary statistics confirm that a simple linear model only explains about half of the variance in this association (R²adj = 50,8%).

Having confirmed our suspicion that there may be some non-linearity involved, we can turn to more powerful diagnostics to determine more conclusively what is happening.

```{r}
plot(defaultfit)
```

The plot of residuals against fitted values clearly resembles a parabola more than a straight line expected of a dataset with good linearity. Thus we strongly suspect that there is a large deviation from our assumption of linearity.

The Q-Q plot appears much less concerning, but there is still a notable parabolic relationship. The tails of the plot are clearly edging upwards, especially in the top half, where many data points of concern are located. Data point number 96 is particularly problematic. The Q-Q plot thus clearly indicates a small departure from the assumption of normality.

The scale-location plot again demonstrates a strong parabolic pattern, although there is a good spread of points - but again the problematic data points at the higher end are highlighted. Thus we strongly suspect that the residuals are not spread equally along the predictor ranges, and thus the assumption of homoscedasticity is violated as well.

The last diagnostic plot, residuals against leverage, shows that data point 96 is indeed a very influential case, and thus we would probably alter our model significantly if we exluded it from analysis. Data point 1 is also relatively influential cases.

Thus we can conclude rather confidently that most of the assumptions underlying linearity do not hold up very well for this association.


# Nonlinear modelling

## Polynomial regression

The first model we constructed was a polynomial regression. We will do this up to the 9th degree polynomial, since this equals ten degrees of freedom.

```{r}
poly_list = list()
MSE_list = list()
for (i in 1:9){
  model = lm(Cscore~poly(lpsa,i), data=prostate, subset=sample)
  poly_list = c(poly_list, list(model))
  mean = mean((Cscore-predict(model,prostate))[-sample]^2)
  MSE_list = c(MSE_list, list(mean))
}
```

As can be seen, the model that has the best test MSE is a third degree polynomial model. We can further confirm this using an ANOVA method for picking the best model.

```{r}
eval(parse(text=paste("anova(",paste("poly_list[[",1:length(poly_list),"]]",sep="",collapse=","),")")))
```

Model 3 is a significant improvement over Model 2, whereas Model 4 gives no significant improvement under a 0.05 significance threshold. Thus a second or third degree polynomial model provides a good fit to the data, and using our previous test MSE result we can pick the third degree polynomial as the best model with some confidence.

```{r}
best_poly_model = lm(Cscore~poly(lpsa, 3), data = prostate)

lpsalims = range(lpsa)
lpsa.grid = seq(lpsalims[1], lpsalims[2], length.out = 50)
prediction = predict(fit, newdata = list(lpsa = lpsa.grid), se = T)
SE.bands = cbind(prediction$fit+2*prediction$se.fit, prediction$fit-2*prediction$se.fit)

plot(lpsa, Cscore, xlim=lpsalims)
title("Third degree polynomial")
lines(lpsa.grid, prediction$fit)
matlines(lpsa.grid, SE.bands, lwd=1, col="red", lty=2)
```

## Cubic splines

The next regression model we construct is a cubic spline. Here, we start from four degrees of freedom, since a cubic spline has three polynomial terms (first, second and third order) and at least one knot. We let the knots be put at uniform quantiles.

```{r}
library(splines)

cubic_list = c()
MSE_list = c()
for (i in 4:10){
  model = lm(Cscore~bs(lpsa, df = i),data=prostate, subset=sample)
  cubic_list = c(cubic_list, model)
  mean = mean((Cscore - predict(model, prostate))[-sample]^2)
  MSE_list = c(MSE_list, list(mean))
}
best_cubic_model = cubic_list[[which.min(MSE_list)]]
best_cubic_model
```

```{r}
best_cubic = lm(Cscore~bs(lpsa, df = 4),data=prostate, subset=sample)

lpsalims = range(lpsa)
lpsa.grid = seq(lpsalims[1], lpsalims[2], length.out = 50)
prediction = predict(fit, newdata = list(lpsa = lpsa.grid), se = T)
SE.bands = cbind(prediction$fit+2*prediction$se.fit, prediction$fit-2*prediction$se.fit)

plot(lpsa, Cscore, xlim=lpsalims)
title("Single knot cubic spline")
lines(lpsa.grid, prediction$fit)
matlines(lpsa.grid, SE.bands, lwd=1, col="red", lty=2)
```


## Smoothing Splines

The last model we construct is a smoothing spline. Since we have duplicated values present in lpsa, we can use generalised cross validation to arrive at the best possible value of the tuning parameter, with best defined as the lowest cross-validated RSS.

```{r}
fit = smooth.spline(lpsa, Cscore, cv = F)
fit$df
```

Unfortunately the best possible degrees of freedom value here is quite large, and we would like to constrain ourselves to a maximum of 10 degrees of freedom.

```{r}
lpsalims = range(lpsa)
lpsa.grid = seq(lpsalims[1], lpsalims[2], length.out = 50)
prediction = predict(fit, newdata = list(lpsa = lpsa.grid))

smooth_list = c()
MSE_list = c()
for (i in 2:10){
  model = smooth.spline(train$lpsa, train$Cscore,  df= i)
  smooth_list = c(smooth_list, list(model))
  lpsalims = range(train$lpsa)
  lpsa.grid = seq(lpsalims[1], lpsalims[2], length.out = length(Cscore))
  prediction = predict(model, newdata = list(lpsa = lpsa.grid))
  mean = mean((Cscore - prediction$y)[-sample]^2)
  MSE_list = c(MSE_list, mean)
}
best_smooth_model = smooth_list[[which.min(MSE_list)]]
##smoothspline_CV = smooth.spline( Cscore[sample],lpsa[sample], cv = TRUE)
##mean((Cscore-predict(smoothspline_CV, prostate))[-sample]^2)
plot(lpsa, Cscore)
lines(best_smooth_model)
fit2 = smooth.spline(lpsa, Cscore, cv=F)
lines(fit2)

```

## Comparison of all the models with the linear model

At last, we compare all the models to see which one gives us the best test error.

```{r}
linear.mse = mean((Cscore-predict(defaultfit, prostate))[-sample]^2)
poly.mse = mean((Cscore-predict(best_poly_model, prostate))[-sample]^2)
cubic.mse =  mean((Cscore-predict(best_cubic, prostate))[-sample]^2)
#smooth.mse = mean((Cscore-predict(best_smooth_model, prostate))[-sample]^2)
linear.mse
poly.mse
cubic.mse

```
These are the test mean square errors for the linear, polynomial and cubic spline models respectively. The polynomial regression and cubic spline are both better models than the linear regression, and the polynomial regression is still a lot better than the spline with a test MSE error of 260. 

We are a bit surprised that the polynomial regression is the best model, since it is a cubic model and the cubic spline also uses a cubic model, in this case with one knot. We think that the cause of this difference is that the knot adds too much variance in our case. 



# General additive model with all variables
```{r}
library(gam)
gam.all = gam(Cscore~s(lcavol,5)+s(lweight,5)+s(age,5)+s(lbph,5)+s(lcp,5)+s(lpsa,5)+svi, data = train)
plot(gam.all)
gam.all.mse = mean((Cscore-predict(gam.all, prostate))[-sample]^2)
gam.all.mse
summary(gam.all)
```
If we look at the summary of the full GAM model, we can see that the predictor variables are all significant, or borderline significant, except for svi, because it is a categorical variable. This could be expected, since the model we use is quite flexible. 

We also looked at the mean test error, which is 570 for this model. this is quite a bit lower than the forward selected model, which had a test MSE of 803. So it seems like a good model for this dataset.

We can also see that the model is not as smooth near the edges for some of the predictors. This is because of the sparseness of the data near the outliers. All in all, we think that this model is good.

# Backwards selection of the General Additive Model

```{r}
getAIC <- function(data, nbloops){
namelist = c()
for (i in 1:nbloops){
  name = paste(data,i,sep ="")
  namelist = c(namelist,name)
}
aiclist = lapply(namelist, function(x)get(x))
aiclist
}

gam.5 = gam(Cscore~s(lcavol,5)+s(lweight,5)+s(lcp,5)+s(lpsa,5)+svi, data = train)$aic
gam.4.1 = gam(Cscore~+s(lweight,5)+s(lcp,5)+s(lpsa,5)+svi, data = train)$aic
gam.4.2 = gam(Cscore~s(lcavol,5)+s(lcp,5)+s(lpsa,5)+svi, data = train)$aic
gam.4.3 = gam(Cscore~s(lcavol,5)+s(lweight,5)+s(lpsa,5)+svi, data = train)$aic
gam.4.4 = gam(Cscore~s(lcavol,5)+s(lweight,5)+s(lcp,5)+svi, data = train)$aic
gam.4.5 = gam(Cscore~s(lcavol,5)+s(lweight,5)+s(lcp,5)+s(lpsa,5), data = train)$aic

aic_4 = getAIC("gam.4.",5)
gam.4 = gam.4.2
gam.3.1 = gam(Cscore~s(lcp,5)+s(lpsa,5)+svi, data = train)$aic
gam.3.2 = gam(Cscore~s(lcavol,5)+s(lpsa,5)+svi, data = train)$aic
gam.3.3 = gam(Cscore~s(lcavol,5)+s(lcp,5)+svi, data = train)$aic
gam.3.4 = gam(Cscore~s(lcavol,5)+s(lcp,5)+s(lpsa,5), data = train)$aic
aic_3 = getAIC("gam.3.",4)

gam.3 = gam.3.1
gam.2.1 = gam(Cscore~s(lpsa,5)+svi, data = train)$aic
gam.2.2 = gam(Cscore~s(lcp,5)+svi, data = train)$aic
gam.2.3 = gam(Cscore~s(lcp,5)+s(lpsa,5), data = train)$aic
aic_2 = getAIC("gam.2.", 3)

gam.2 = gam.2.3
gam.1.1 = gam(Cscore~s(lpsa,5), data = train)$aic
gam.1.2 = gam(Cscore~s(lcp,5), data = train)$aic
aic_1 = getAIC("gam.1.",2)
aic_1
gam.1 = gam.1.1
```
To select the best backwards selection model, we first manually perform backwards selection on the models. The best model with a specified number of variables is selected by using the AIC, we started for every continuous variable with the degrees of freedom set to five. We use AIC, even though the algorithm in ISLR tells us to use the R² value, since this value is readily available in the summary and we are not sure how good of a measure R² is for a GAM.
```{r}
gam5 = gam(Cscore~s(lcavol,5)+s(lweight,5)+s(lcp,5)+s(lpsa,5)+svi, data = train)
gam4 = gam(Cscore~s(lcavol,5)+s(lcp,5)+s(lpsa,5)+svi, data = train)
gam3 = gam(Cscore~s(lcp,5)+s(lpsa,5)+svi, data = train)
gam2 = gam(Cscore~s(lcp,5)+s(lpsa,5), data = train)
gam1 = gam(Cscore~s(lpsa,5), data = train)

gam5.mse = mean((Cscore-predict(gam5, prostate))[-sample]^2)
gam4.mse = mean((Cscore-predict(gam4, prostate))[-sample]^2)
gam3.mse = mean((Cscore-predict(gam3, prostate))[-sample]^2)
gam2.mse = mean((Cscore-predict(gam2, prostate))[-sample]^2)
gam1.mse = mean((Cscore-predict(gam1, prostate))[-sample]^2)
testerrs = c(gam5.mse,gam4.mse,gam3.mse,gam2.mse,gam1.mse)
testerrs

```
To select the best model out of all the models with different numbers of variables, we use cross-validation. The model with two variables has the lowest test error, so we used this model for the rest of our analysis, to vary the degrees of freedom of the predictor variables.

```{r}
MSE = 5000
for (i in 1:15){
  for (j in 1:15){
    model = gam(Cscore~s(lcp,i)+s(lpsa,j), data = train)
    mse = mean((Cscore-predict(model, prostate))[-sample]^2)
    if (mse < MSE){
      i.select = i
      j.select = j
      MSE = mse
    }
  }
}
i.select
j.select
backwards_selected = gam(Cscore~s(lcp,2)+s(lpsa,3), data = train)
summary(backwards_selected)
```
We select the model with the lowest test mean squared error as the best model. The degrees of freedom of both predictor variables is varied between one and fifteen. As we can see, the best model has two degrees of freedom for lcp, and three degrees of freedom for lpsa.

## Comparison to linear models

